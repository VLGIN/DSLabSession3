{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "international-offer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import random\n",
    "tf.disable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-coupon",
   "metadata": {},
   "source": [
    "# Lớp MLP:\n",
    "## build_graph\n",
    "Xây dựng mô hình multi-layer perceptron\n",
    "## trainer:\n",
    "Phương thức tối ưu (Adam optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ignored-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size # kích thước vector đầu vào\n",
    "        self._hidden_size = hidden_size # kích thước của tầng ẩn\n",
    "    def build_graph(self):\n",
    "        NUM_CLASSES = 20 # kích thước vector đầu ra\n",
    "        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\n",
    "        \n",
    "        weights_1 = tf.get_variable(\n",
    "            name='weights_input_hidden',\n",
    "            shape=(self._vocab_size, self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        \n",
    "        biases_1 = tf.get_variable(\n",
    "            name='biases_input_hidden',\n",
    "            shape=(self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        \n",
    "        weights_2 = tf.get_variable(\n",
    "            name='weights_hidden_output',\n",
    "            shape=(self._hidden_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2018),\n",
    "        )\n",
    "        \n",
    "        biases_2 = tf.get_variable(\n",
    "            name='biases_hidden_output',\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2018)\n",
    "        )\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES,\n",
    "                                   dtype=tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot,\n",
    "                                                      logits=logits)\n",
    "        loss=tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis = 1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        \n",
    "        return predicted_labels, loss\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "seeing-harris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leo/anaconda3/envs/internship/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../session 2/dataset/words_idfs.txt') as f: # path dẫn đến file idf của tập vocabulary\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "\n",
    "# Khởi tạo 1 đối tượng của lớp MLP\n",
    "mlp = MLP(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=50\n",
    ")\n",
    "\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "train_op = mlp.trainer(loss=loss, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-advantage",
   "metadata": {},
   "source": [
    "# Lớp DataReader: Đọc dữ liệu\n",
    "## next_batch: Đọc dữ liệu theo từng batch với bath_size được chỉ định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "involved-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        # Đọc dữ liệu vào\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines() \n",
    "        \n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        \n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split('<fff>')\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            \n",
    "            # Tạo vector tf_idf\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "        \n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        \n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "        \n",
    "        if end + self._batch_size > len(self._data):\n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2018)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "            \n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "realistic-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đọc dữ liệu\n",
    "def load_dataset():\n",
    "    train_data_reader = DataReader(\n",
    "        data_path='../session 2/dataset/train_tf_idf.txt', # path dẫn đến file dữ liệu train\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    test_data_reader = DataReader(\n",
    "        data_path='../session 2/dataset/test_tf_idf.txt', # path dẫn đến file dữ liệu test\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coordinate-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu các tham số mô hình tại 1 epoch\n",
    "def save_paramters(name, value, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = ','.join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = '\\n'.join([','.join([str(number)\n",
    "                                          for number in value[row]])\n",
    "                                for row in range(value.shape[0])])\n",
    "    with open('saved_paras/' + filename, 'w+') as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dried-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load số liệu của tham số mô hình\n",
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "    with open('saved_paras/' + filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(',')]\n",
    "    else:\n",
    "        value = [[float(number) for number in lines[row].split(',')]\n",
    "                for row in range(len(lines))]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-elimination",
   "metadata": {},
   "source": [
    "# Train mô hình\n",
    "step = 20000\n",
    "Tương đương với khoảng 80 epoch\n",
    "\n",
    "Mỗi step tương đương với 1 batch dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_data_reader, test_data_reader = load_dataset()\n",
    "    step, MAX_STEP = 0, 20000\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while step < MAX_STEP:\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        plabels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict={\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        step += 1\n",
    "        print('step: {}, loss: {}'.format(step, loss_eval))\n",
    "        \n",
    "        # Lưu tham số mô hình sau mỗi epoch\n",
    "        if train_data_reader._batch_id == 0:\n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            for variable in trainable_variables:\n",
    "                save_paramters(\n",
    "                                name = variable.name,\n",
    "                                value = variable.eval(),\n",
    "                                epoch = train_data_reader._num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-distribution",
   "metadata": {},
   "source": [
    "# Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_reader, test_data_reader = load_dataset()\n",
    "with tf.Session() as sess:\n",
    "    for epoch in range(10, 81, 20):\n",
    "        trainable_varibales = tf.trainable_variables()\n",
    "        for variable in trainable_variables:\n",
    "            saved_value = restore_parameters(variable.name, epoch)\n",
    "            assign_op = variable.assign(saved_value)\n",
    "            sess.run(assign_op)\n",
    "        num_true = 0\n",
    "        while True:\n",
    "            test_data, test_labels = test_data_reader.next_batch()\n",
    "            test_plabels_eval = sess.run(\n",
    "                predicted_labels,\n",
    "                feed_dict = {mlp._X: test_data, mlp._real_Y:test_labels}\n",
    "            )\n",
    "            matches = np.equal(test_plabels_eval, test_labels)\n",
    "            num_true += np.sum(matches.astype(float))\n",
    "            if test_data_reader._batch_id == 0:\n",
    "                break\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "        print(\"Accuracy: {}\".format(num_true / len(test_data_reader._data)))\n",
    "#epoch: 10\n",
    "#Accuracy: 0.7823951141795008\n",
    "#epoch: 20\n",
    "#Accuracy: 0.7846521508231545\n",
    "#epoch: 30\n",
    "#Accuracy: 0.7561072756240043\n",
    "#epoch: 40\n",
    "#Accuracy: 0.7550451407328731\n",
    "#epoch: 50\n",
    "#Accuracy: 0.7549123738714817\n",
    "#epoch: 60\n",
    "#Accuracy: 0.7551779075942645\n",
    "#epoch: 70\n",
    "#Accuracy: 0.7465480616038237\n",
    "#epoch: 80\n",
    "#Accuracy: 0.7484067976633032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-difference",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
